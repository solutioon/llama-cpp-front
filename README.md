# llama-cpp-front

This is a front for working against a deployed llama-cpp-python backend, using the API.

It can be deployed directly or in docker or K8s.

Feel free to use it at your own risk.
